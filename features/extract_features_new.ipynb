{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from train/valid/test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\wangs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from features.cosine_similarity import cosine_similarity\n",
    "from features.content_features import *\n",
    "from features.user_features import *\n",
    "from features.structural_features import *\n",
    "from features.sentiment_features import *\n",
    "# from data_helper import *\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "import pandas as pd\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = '../data/Experiment/Train1.tsv'\n",
    "valid_file = '../data/Experiment/Valid1.tsv'\n",
    "test_file = '../data/Experiment/Test1.tsv'\n",
    "data_file = '../data/Experiment/Data1.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tf_idf_dict(idf_file):\n",
    "    term_to_idf_dict = {}\n",
    "\n",
    "    with open(idf_file, encoding='utf-8') as fin:\n",
    "        next(fin)\n",
    "        for line in fin:\n",
    "#             print(line)\n",
    "            tokens = line.split('\\t')\n",
    "            term_to_idf_dict[tokens[1]] = float(tokens[2].strip('\\n'))\n",
    "\n",
    "    return term_to_idf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_file = '../data/idf.tsv'\n",
    "\n",
    "train_feat_file = '../data/Experiment/train_features.csv'\n",
    "valid_feat_file = '../data/Experiment/valid_features.csv'\n",
    "test_feat_file = '../data/Experiment/test_features.csv'\n",
    "\n",
    "pos_file = '../data/positive-words.txt'\n",
    "neg_file = '../data/negative-words.txt'\n",
    "term_to_idf_dict = init_tf_idf_dict(idf_file)\n",
    "pos_dict, neg_dict = load_sentiment_lexicon(pos_file, neg_file)\n",
    "labelTable={1:'OQ',2:'RQ',3:'CQ',4:'FD',5:'FQ',\\\n",
    "            6:'IR',7:'PA',8:'PF',9:'NF',10:'GG',11:'JK',12:'O'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(in_file):\n",
    "    with open(in_file, encoding='cp437') as fin:\n",
    "        utterances = []\n",
    "        labels = []\n",
    "        uoas = []\n",
    "        dialogIDs = []\n",
    "        # skip the first line\n",
    "        next(fin)\n",
    "        for line in fin:\n",
    "            if line != '\\n':\n",
    "                tokens = line.strip().split('\\t')\n",
    "                utterances.append(tokens[1])\n",
    "    #                 print(tokens[2])\n",
    "                dialogIDs.append(tokens[2])\n",
    "                labels.append(tokens[3])\n",
    "                uoas.append(tokens[4])\n",
    "\n",
    "\n",
    "    # extract features\n",
    "    label_features=[]                \n",
    "    # content based features\n",
    "    title_sim, init_sim, thread_sim = cosine_similarity(\"\", utterances, term_to_idf_dict)\n",
    "    qm = question_mark(utterances)\n",
    "    dup = duplicate(utterances)\n",
    "    wh = W5H1(utterances)\n",
    "\n",
    "    # structural features\n",
    "    abs_pos = [idx + 1 for idx in range(len(utterances))]\n",
    "    norm_pos = [pos / len(utterances) for pos in abs_pos]\n",
    "    length, unique_length, unique_stemmed_length = post_length(utterances)\n",
    "\n",
    "    # user features\n",
    "    #                 ua = user_auth(affiliations)\n",
    "    is_starter = [1 if uoa == 'User' else 0 for uoa in uoas]\n",
    "\n",
    "    # sentiment based features\n",
    "    thx = thank(utterances)\n",
    "    exclam_mark = exclamation_mark(utterances)\n",
    "    vf = ve_feedback(utterances)\n",
    "    ss = sentiment_scores(utterances)\n",
    "    lexicon_counts = lexicon(utterances, pos_dict, neg_dict)\n",
    "\n",
    "    for i, utterance in enumerate(utterances):\n",
    "        label_feature = '{}\\t{:.4f} {:.4f} {} {} {} {} {:.4f} {} {} {} {} {} {} {} {} {}\\n'.format(\n",
    "            labels[i],\n",
    "            init_sim[i],\n",
    "            thread_sim[i],\n",
    "            qm[i],\n",
    "            dup[i],\n",
    "            ' '.join(wh[i]),\n",
    "            abs_pos[i],\n",
    "            norm_pos[i],\n",
    "            length[i],\n",
    "            unique_length[i],\n",
    "            unique_stemmed_length[i],\n",
    "            is_starter[i],\n",
    "            thx[i],\n",
    "            exclam_mark[i],\n",
    "            vf[i],\n",
    "            ' '.join(ss[i]),\n",
    "            ' '.join(lexicon_counts[i]),\n",
    "        )\n",
    "        label_features += [label_feature]\n",
    "\n",
    "    output = pd.DataFrame()\n",
    "    output['utterances'] = utterances\n",
    "    output['dialogIDs'] = dialogIDs\n",
    "    output['label_features'] = label_features\n",
    "    output.head()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = extract_features(test_file)\n",
    "Train = extract_features(train_file)\n",
    "Valid = extract_features(valid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test.to_csv(test_feat_file, sep='\\t', index=False, header=False)\n",
    "Test.to_csv(test_feat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.to_csv(train_feat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid.to_csv(valid_feat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

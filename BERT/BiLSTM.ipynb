{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPNaexvyLxuSPg0EbW+anDc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtsyang/UserIntentPrediction/blob/BERT/BERT/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot0Ha--FR-S1",
        "colab_type": "code",
        "outputId": "8d6a04c8-3fbd-4da0-c9c8-35a68b285549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6DT7DNwSNn9",
        "colab_type": "code",
        "outputId": "f256ca90-9faa-440c-c28e-f894127403be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/UserIntentPrediction'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/UserIntentPrediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De681umQRanU",
        "colab_type": "code",
        "outputId": "1f55e296-badd-4211-e9ee-ba3d22096df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
        "from tensorflow.keras.layers  import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing import text, sequence\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection  import train_test_split\n",
        "import pickle\n",
        "import sklearn\n",
        "from tensorflow.keras.utils import multi_gpu_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "print('Tensorflow Version:',tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow Version: 2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIpXSpeYR4rx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_MODELS = 1\n",
        "BATCH_SIZE = 32\n",
        "LSTM_UNITS = 256\n",
        "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
        "EPOCHS = 16\n",
        "MAX_LEN = 1259\n",
        "N_CHANNELS=768\n",
        "N_CLASS=12"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lcdmN7BSHGl",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p2_0TqoR9ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train=pd.read_csv('data/Train_Preprocessing.csv').reset_index(drop=True)\n",
        "Valid=pd.read_csv('data/Valid_Preprocessing.csv').reset_index(drop=True)\n",
        "Test=pd.read_csv('data/Test_Preprocessing.csv').reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjliEKeAhO4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, pdDataFrame, dbName, labels=['oQ', 'RQ', 'CQ', 'FD', 'FQ', 'IR', 'PA', 'PF', 'NF', 'GG', 'JK', 'O'],\\\n",
        "                 batch_size=BATCH_SIZE, dim=MAX_LEN, n_channels=N_CHANNELS,\\\n",
        "                 n_classes=N_CLASS, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = pdDataFrame\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.dbName=dbName\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = self.list_IDs.iloc[indexes,:]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp.reset_index(drop=True))\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n",
        "        y = np.zeros((self.batch_size,self.n_classes), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i in range(len(list_IDs_temp)):\n",
        "            utterenceID=list_IDs_temp.loc[i,'id']\n",
        "            diaglogID=list_IDs_temp.loc[i,'diaglogID']\n",
        "            try:\n",
        "              temp=np.load('BERT/vector/'+self.dbName+'_'+str(utterenceID)+'_'+str(diaglogID)+'.npy')\n",
        "              X[i,0:temp.shape[0],:] =temp \n",
        "              del temp\n",
        "            except:\n",
        "              print('Faile to load the data: BERT/vector/'+self.dbName+'_'+str(utterenceID)+'_'+str(diaglogID)+'.npy')\n",
        "            # Store sample\n",
        "            # Store class\n",
        "            y[i,:] = np.array(list_IDs_temp.iloc[i,0:12])\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ijHEnaK7opB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_generator = DataGenerator(Train,'Train')\n",
        "validation_generator = DataGenerator(Valid,'Valid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mz_N6c7mEx",
        "colab_type": "text"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfKQSOUM5saC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "    inputs = Input(shape=(MAX_LEN,N_CHANNELS))\n",
        "    x = SpatialDropout1D(0.2)(inputs)\n",
        "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
        "\n",
        "    hidden = concatenate([\n",
        "        GlobalMaxPooling1D()(x),\n",
        "        GlobalAveragePooling1D()(x),\n",
        "    ])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
        "    result = Dense(N_CLASS, activation='sigmoid')(hidden)\n",
        "    #aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
        "    \n",
        "    #model = Model(inputs=words, outputs=[result, aux_result])\n",
        "    model = Model(inputs=inputs, outputs=result)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[tf.keras.metrics.MeanIoU(num_classes=N_CLASS),tf.keras.metrics.binary_accuracy])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N37opYym6CCC",
        "colab_type": "code",
        "outputId": "75898ea9-a0d8-4f97-b3f7-9b5d2200ccb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 1259, 768)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_2 (SpatialDro (None, 1259, 768)    0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 1259, 512)    2099200     spatial_dropout1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 1259, 512)    1574912     bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_2 (GlobalM (None, 512)          0           bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 512)          0           bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1024)         0           global_max_pooling1d_2[0][0]     \n",
            "                                                                 global_average_pooling1d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1024)         1049600     concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 1024)         0           concatenate_2[0][0]              \n",
            "                                                                 dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 1024)         1049600     add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 1024)         0           add_4[0][0]                      \n",
            "                                                                 dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 12)           12300       add_5[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 5,785,612\n",
            "Trainable params: 5,785,612\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4To3UbV67kft",
        "colab_type": "code",
        "outputId": "21d4f27c-b428-46b2-a768-4994763d0560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='BERT/LSTM.h5', verbose=1, save_best_only=True)\n",
        "model.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=EPOCHS,\n",
        "    verbose=1,\n",
        "    callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 59\n",
            "Epoch 1/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.2449 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9066\n",
            "Epoch 00001: val_loss improved from inf to 0.20306, saving model to BERT/LSTM.h5\n",
            "251/251 [==============================] - 126s 500ms/step - loss: 0.2449 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9066 - val_loss: 0.2031 - val_mean_io_u_1: 0.4419 - val_binary_accuracy: 0.9200\n",
            "Epoch 2/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1975 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9239\n",
            "Epoch 00002: val_loss improved from 0.20306 to 0.18470, saving model to BERT/LSTM.h5\n",
            "251/251 [==============================] - 123s 490ms/step - loss: 0.1975 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9239 - val_loss: 0.1847 - val_mean_io_u_1: 0.4419 - val_binary_accuracy: 0.9280\n",
            "Epoch 3/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1832 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9304\n",
            "Epoch 00003: val_loss improved from 0.18470 to 0.17644, saving model to BERT/LSTM.h5\n",
            "251/251 [==============================] - 123s 489ms/step - loss: 0.1832 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9304 - val_loss: 0.1764 - val_mean_io_u_1: 0.4418 - val_binary_accuracy: 0.9305\n",
            "Epoch 4/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1707 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9354\n",
            "Epoch 00004: val_loss improved from 0.17644 to 0.17212, saving model to BERT/LSTM.h5\n",
            "251/251 [==============================] - 123s 489ms/step - loss: 0.1707 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9354 - val_loss: 0.1721 - val_mean_io_u_1: 0.4419 - val_binary_accuracy: 0.9343\n",
            "Epoch 5/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1580 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9393\n",
            "Epoch 00005: val_loss improved from 0.17212 to 0.17055, saving model to BERT/LSTM.h5\n",
            "251/251 [==============================] - 123s 490ms/step - loss: 0.1580 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9393 - val_loss: 0.1705 - val_mean_io_u_1: 0.4418 - val_binary_accuracy: 0.9342\n",
            "Epoch 6/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1478 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9436\n",
            "Epoch 00006: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 122s 484ms/step - loss: 0.1478 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9436 - val_loss: 0.1750 - val_mean_io_u_1: 0.4421 - val_binary_accuracy: 0.9355\n",
            "Epoch 7/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1368 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9476\n",
            "Epoch 00007: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 121s 482ms/step - loss: 0.1368 - mean_io_u_1: 0.4400 - binary_accuracy: 0.9476 - val_loss: 0.1786 - val_mean_io_u_1: 0.4419 - val_binary_accuracy: 0.9333\n",
            "Epoch 8/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1252 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9509\n",
            "Epoch 00008: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 122s 485ms/step - loss: 0.1252 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9509 - val_loss: 0.1801 - val_mean_io_u_1: 0.4420 - val_binary_accuracy: 0.9307\n",
            "Epoch 9/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.1100 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9575\n",
            "Epoch 00009: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 122s 485ms/step - loss: 0.1100 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9575 - val_loss: 0.1825 - val_mean_io_u_1: 0.4418 - val_binary_accuracy: 0.9338\n",
            "Epoch 10/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.0960 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9634\n",
            "Epoch 00010: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 121s 483ms/step - loss: 0.0960 - mean_io_u_1: 0.4401 - binary_accuracy: 0.9634 - val_loss: 0.2197 - val_mean_io_u_1: 0.4419 - val_binary_accuracy: 0.9268\n",
            "Epoch 11/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.0828 - mean_io_u_1: 0.4408 - binary_accuracy: 0.9677\n",
            "Epoch 00011: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 122s 486ms/step - loss: 0.0828 - mean_io_u_1: 0.4408 - binary_accuracy: 0.9677 - val_loss: 0.2195 - val_mean_io_u_1: 0.4431 - val_binary_accuracy: 0.9306\n",
            "Epoch 12/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.0709 - mean_io_u_1: 0.4414 - binary_accuracy: 0.9733\n",
            "Epoch 00012: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 121s 483ms/step - loss: 0.0709 - mean_io_u_1: 0.4414 - binary_accuracy: 0.9733 - val_loss: 0.2286 - val_mean_io_u_1: 0.4426 - val_binary_accuracy: 0.9303\n",
            "Epoch 13/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.0566 - mean_io_u_1: 0.4445 - binary_accuracy: 0.9781\n",
            "Epoch 00013: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 121s 484ms/step - loss: 0.0566 - mean_io_u_1: 0.4445 - binary_accuracy: 0.9781 - val_loss: 0.2721 - val_mean_io_u_1: 0.4461 - val_binary_accuracy: 0.9256\n",
            "Epoch 14/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.0478 - mean_io_u_1: 0.4541 - binary_accuracy: 0.9816\n",
            "Epoch 00014: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 121s 483ms/step - loss: 0.0478 - mean_io_u_1: 0.4541 - binary_accuracy: 0.9816 - val_loss: 0.2824 - val_mean_io_u_1: 0.4517 - val_binary_accuracy: 0.9229\n",
            "Epoch 15/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.0397 - mean_io_u_1: 0.4603 - binary_accuracy: 0.9850\n",
            "Epoch 00015: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 122s 486ms/step - loss: 0.0397 - mean_io_u_1: 0.4603 - binary_accuracy: 0.9850 - val_loss: 0.3070 - val_mean_io_u_1: 0.4673 - val_binary_accuracy: 0.9262\n",
            "Epoch 16/16\n",
            "251/251 [==============================] - ETA: 0s - loss: 0.0322 - mean_io_u_1: 0.4711 - binary_accuracy: 0.9883\n",
            "Epoch 00016: val_loss did not improve from 0.17055\n",
            "251/251 [==============================] - 121s 482ms/step - loss: 0.0322 - mean_io_u_1: 0.4711 - binary_accuracy: 0.9883 - val_loss: 0.3219 - val_mean_io_u_1: 0.4515 - val_binary_accuracy: 0.9240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1200524470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMfSJxN--xs1",
        "colab_type": "text"
      },
      "source": [
        "## Prediction\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZwqpZmP_pVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class testDataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, pdDataFrame, dbName, labels=['oQ', 'RQ', 'CQ', 'FD', 'FQ', 'IR', 'PA', 'PF', 'NF', 'GG', 'JK', 'O'],\\\n",
        "                 batch_size=25, dim=MAX_LEN, n_channels=N_CHANNELS,\\\n",
        "                 n_classes=N_CLASS, shuffle=False):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = pdDataFrame\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.dbName=dbName\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = self.list_IDs.iloc[indexes,:]\n",
        "\n",
        "        # Generate data\n",
        "        X = self.__data_generation(list_IDs_temp.reset_index(drop=True))\n",
        "\n",
        "        return X\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.zeros((self.batch_size, self.dim, self.n_channels))\n",
        "        #y = np.zeros((self.batch_size,self.n_classes), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i in range(len(list_IDs_temp)):\n",
        "            utterenceID=list_IDs_temp.loc[i,'id']\n",
        "            diaglogID=list_IDs_temp.loc[i,'diaglogID']\n",
        "            try:\n",
        "              temp=np.load('BERT/vector/'+self.dbName+'_'+str(utterenceID)+'_'+str(diaglogID)+'.npy')\n",
        "              X[i,0:temp.shape[0],:] =temp \n",
        "              del temp\n",
        "            except:\n",
        "              print('Faile to load the data: BERT/vector/'+self.dbName+'_'+str(utterenceID)+'_'+str(diaglogID)+'.npy')\n",
        "            # Store sample\n",
        "            # Store class\n",
        "            #y[i,:] = np.array(list_IDs_temp.iloc[i,0:12])\n",
        "\n",
        "        return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgnTSkm2_EBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_generator=testDataGenerator(Test,'Test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MneDN8jV-spA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict_generator(test_generator)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfKCnqyr_dH-",
        "colab_type": "code",
        "outputId": "2c14f987-f64c-44c5-de78-0e5b752d2806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "IOU=0\n",
        "for i in range(len(Test)):\n",
        "  pred=prediction[i]\n",
        "  labels=np.array(Test.iloc[i,0:12])\n",
        "  ioU=np.sum((pred>=0.5)&(labels==1))/np.sum( (pred>=0.5) | (labels==1))\n",
        "  IOU+=ioU\n",
        "IOU=IOU/len(Test)\n",
        "print(IOU)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5801081081081079\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DavG4UnTDpVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IOU=0\n",
        "for i in range(len(Test)):\n",
        "  pred=prediction[i]\n",
        "  labels=np.array(Test.iloc[i,0:12])\n",
        "  ioU=np.sum((pred==labels)/12\n",
        "  IOU+=ioU\n",
        "IOU=IOU/len(Test)\n",
        "print(IOU)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
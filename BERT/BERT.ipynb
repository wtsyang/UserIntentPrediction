{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "BERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wtsyang/UserIntentPrediction/blob/BERT/BERT/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUdcSlieroSb",
        "colab_type": "code",
        "outputId": "ce1eef73-aba9-45f7-c356-0a32d553ee22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive # import drive from google colab\n",
        "\n",
        "ROOT = \"/content/drive\"     # default location for the drive\n",
        "print(ROOT)                 # print content of ROOT (Optional)\n",
        "\n",
        "drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61qmnPSIuOVZ",
        "colab_type": "code",
        "outputId": "5cb00c89-c5f3-494b-c5e7-1fb2723fc62c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd '/content/drive/My Drive/UserIntentPrediction'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/UserIntentPrediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G-zbgh8ANPK",
        "colab_type": "code",
        "outputId": "964bdc6e-2c9d-49c5-ea65-6b41b53aa033",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n",
        "!pip install tools"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 16.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.38.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.12.27)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.15.27)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.27->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n",
            "Collecting tools\n",
            "  Downloading https://files.pythonhosted.org/packages/de/20/2a2dddb083fd0ce56b453cf016768b2c49f3c0194090500f78865b7d110c/tools-0.1.9.tar.gz\n",
            "Collecting pytils\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/c1/12b556b5bb393ce5130d57af862d045f57fee764797c0fe837e49cb2a5da/pytils-0.3.tar.gz (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tools) (1.12.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from tools) (4.2.6)\n",
            "Building wheels for collected packages: tools, pytils\n",
            "  Building wheel for tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tools: filename=tools-0.1.9-cp36-none-any.whl size=46760 sha256=97b092dcbd062b4641d19be0256ea0a45f249c117d82c0a78d78b03377aa40f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/9b/1ca7dcb0b9ebfdc23a00c85a0644abb6fb14f9159a0df8e067\n",
            "  Building wheel for pytils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytils: filename=pytils-0.3-cp36-none-any.whl size=40355 sha256=e928549d099217565e5eebb0ba821a9297c0a02ef5072de15e82a2b48e8a8f76\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/f9/dc/4f07d8ee40d9cfca9973b3f4aeff99d0bb69900e5f3dffbf32\n",
            "Successfully built tools pytils\n",
            "Installing collected packages: pytils, tools\n",
            "Successfully installed pytils-0.3 tools-0.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yCqNKujAER8",
        "colab_type": "code",
        "outputId": "7df1824e-fd59-44a2-9b6e-a38fa75f7810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "import pickle\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "from tqdm import tqdm_notebook, trange\n",
        "import os\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
        "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
        "\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tools import *\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print('Torch Version:',torch.__version__)\n",
        "print('NLTK Version:',nltk.__version__)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch Version: 1.4.0\n",
            "NLTK Version: 3.2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmskIfBwP44U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The input data dir. Should contain the .tsv files (or other data files) for the task.\n",
        "DATA_DIR = \"data/\"\n",
        "\n",
        "# Bert pre-trained model selected in the list: bert-base-uncased, \n",
        "# bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n",
        "# bert-base-multilingual-cased, bert-base-chinese.\n",
        "BERT_MODEL = 'bert-base-cased'\n",
        "\n",
        "# The name of the task to train.I'm going to name this 'yelp'.\n",
        "TASK_NAME = 'intentionClassification'\n",
        "\n",
        "# The output directory where the fine-tuned model and checkpoints will be written.\n",
        "OUTPUT_DIR = f'outputs/{TASK_NAME}/'\n",
        "\n",
        "# The directory where the evaluation reports will be written to.\n",
        "REPORTS_DIR = f'reports/{TASK_NAME}_evaluation_report/'\n",
        "\n",
        "# This is where BERT will look for pre-trained models to load parameters from.\n",
        "CACHE_DIR = 'cache/'\n",
        "\n",
        "# The maximum total input sequence length after WordPiece tokenization.\n",
        "# Sequences longer than this will be truncated, and sequences shorter than this will be padded.\n",
        "max_seq_length = 512\n",
        "\n",
        "TRAIN_BATCH_SIZE = 24\n",
        "EVAL_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 1\n",
        "RANDOM_SEED = 42\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "WARMUP_PROPORTION = 0.1\n",
        "OUTPUT_MODE = 'classification'\n",
        "num_labels=12\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR-i51tnZAIQ",
        "colab_type": "text"
      },
      "source": [
        "### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgwoFCv0ZC8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train=pd.read_csv('data/Train_Preprocessing.csv').reset_index(drop=True)\n",
        "Valid=pd.read_csv('data/Valid_Preprocessing.csv').reset_index(drop=True)\n",
        "Test=pd.read_csv('data/Test_Preprocessing.csv').reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHX_0UviY3gX",
        "colab_type": "text"
      },
      "source": [
        "### Create Folder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYNiMqUXQKVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists(REPORTS_DIR) and os.listdir(REPORTS_DIR):\n",
        "        REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
        "        os.makedirs(REPORTS_DIR)\n",
        "if not os.path.exists(REPORTS_DIR):\n",
        "    os.makedirs(REPORTS_DIR)\n",
        "    REPORTS_DIR += f'/report_{len(os.listdir(REPORTS_DIR))}'\n",
        "    os.makedirs(REPORTS_DIR)\n",
        "if os.path.exists(OUTPUT_DIR) and os.listdir(OUTPUT_DIR):\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(OUTPUT_DIR))\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGtoZgcfZwfh",
        "colab_type": "text"
      },
      "source": [
        "### Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs5pCfCbaKFy",
        "colab_type": "code",
        "outputId": "6cf68a04-3543-413c-b140-b506f3477c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "nltk.download('punkt')\n",
        "sentence_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /tmp/tmpu4owv_0q\n",
            "100%|██████████| 213450/213450 [00:00<00:00, 5181134.81B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpu4owv_0q to cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpu4owv_0q\n",
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uWHQe2baBAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Process():\n",
        "  def __init__(self):\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "    nltk.download('punkt')\n",
        "    self.sentence_tokenizer=nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "  def processing(self,text):\n",
        "    # Tokenization\n",
        "    tokenized_text=self.tokenizer.tokenize(text)\n",
        "    assert len(tokenized_text) <= (max_seq_length - 2)\n",
        "    tokenized_text = [\"[CLS]\"] + tokenized_text + [\"[SEP]\"]\n",
        "    indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    \n",
        "     # Sentence Embedding\n",
        "    segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    return tokens_tensor,segments_tensors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRytGo3AYUR0",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag32VLMjQVgH",
        "colab_type": "code",
        "outputId": "dddcf31d-dcbf-4782-c05d-89f14e9d0299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained(BERT_MODEL)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz not found in cache, downloading to /tmp/tmptdkyva2a\n",
            "100%|██████████| 404400730/404400730 [00:05<00:00, 68846733.91B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmptdkyva2a to cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmptdkyva2a\n",
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /tmp/tmp37w0765v\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzvhocPQYZvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0BrZi-Jp0Xn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "7c0e42b4-d68c-4233-929a-8454021ab398"
      },
      "source": [
        "process=Process()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7PO_ncZJbPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "  \n",
        "    dbName='Train'\n",
        "    Target=Train.copy()\n",
        "    for i in range(len(Target)):\n",
        "      sentences=sentence_tokenizer.tokenize(Target.loc[i,'utterance'])\n",
        "      token_vecs_cat = []\n",
        "      for sentence in sentences:\n",
        "        temp=process.processing(sentence)\n",
        "        encoded_layers, _ = model(temp[0], temp[1])\n",
        "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "        # `token_embeddings` is a [tokenSize x layerSize x featureSize] tensor.\n",
        "        # For each token in the sentence...\n",
        "        for token in token_embeddings:\n",
        "            # `token` is a [12 x 768] tensor\n",
        "            # Concatenate the vectors (that is, append them together) from the last \n",
        "            # four layers.\n",
        "            # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "            cat_vec = token[-1]#torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "            # Use `cat_vec` to represent `token`.\n",
        "            token_vecs_cat.append(list(cat_vec.cpu().numpy()))\n",
        "      sentencesTensor=np.array(token_vecs_cat)\n",
        "      utterenceID=Target.loc[i,'id']\n",
        "      diaglogID=Target.loc[i,'diaglogID']\n",
        "      np.save('BERT/vector/'+dbName+'_'+str(utterenceID)+'_'+str(diaglogID)+'.npy',sentencesTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COB-S9wUARn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "  \n",
        "    dbName='Valid'\n",
        "    Target=Valid.copy()\n",
        "    for i in range(len(Target)):\n",
        "      sentences=sentence_tokenizer.tokenize(Target.loc[i,'utterance'])\n",
        "      token_vecs_cat = []\n",
        "      for sentence in sentences:\n",
        "        temp=process.processing(sentence)\n",
        "        encoded_layers, _ = model(temp[0], temp[1])\n",
        "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "        # `token_embeddings` is a [tokenSize x layerSize x featureSize] tensor.\n",
        "        # For each token in the sentence...\n",
        "        for token in token_embeddings:\n",
        "            # `token` is a [12 x 768] tensor\n",
        "            # Concatenate the vectors (that is, append them together) from the last \n",
        "            # four layers.\n",
        "            # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "            cat_vec = token[-1]#torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "            # Use `cat_vec` to represent `token`.\n",
        "            token_vecs_cat.append(list(cat_vec.cpu().numpy()))\n",
        "      sentencesTensor=np.array(token_vecs_cat)\n",
        "      utterenceID=Target.loc[i,'id']\n",
        "      diaglogID=Target.loc[i,'diaglogID']\n",
        "      np.save('BERT/vector/'+dbName+'_'+str(utterenceID)+'_'+str(diaglogID)+'.npy',sentencesTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_HBofhyEsHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()\n",
        "# Predict hidden states features for each layer\n",
        "with torch.no_grad():\n",
        "  \n",
        "    dbName='Test'\n",
        "    Target=Test.copy()\n",
        "    for i in range(len(Target)):\n",
        "      sentences=sentence_tokenizer.tokenize(Target.loc[i,'utterance'])\n",
        "      token_vecs_cat = []\n",
        "      for sentence in sentences:\n",
        "        temp=process.processing(sentence)\n",
        "        encoded_layers, _ = model(temp[0], temp[1])\n",
        "        token_embeddings = torch.stack(encoded_layers, dim=0)\n",
        "        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "        token_embeddings = token_embeddings.permute(1,0,2)\n",
        "        # `token_embeddings` is a [tokenSize x layerSize x featureSize] tensor.\n",
        "        # For each token in the sentence...\n",
        "        for token in token_embeddings:\n",
        "            # `token` is a [12 x 768] tensor\n",
        "            # Concatenate the vectors (that is, append them together) from the last \n",
        "            # four layers.\n",
        "            # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
        "            cat_vec = token[-1]#torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
        "            # Use `cat_vec` to represent `token`.\n",
        "            token_vecs_cat.append(list(cat_vec.cpu().numpy()))\n",
        "      sentencesTensor=np.array(token_vecs_cat)\n",
        "      utterenceID=Target.loc[i,'id']\n",
        "      diaglogID=Target.loc[i,'diaglogID']\n",
        "      np.save('BERT/vector/'+dbName+'_'+str(utterenceID)+'_'+str(diaglogID)+'.npy',sentencesTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6XPD5IREutv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}